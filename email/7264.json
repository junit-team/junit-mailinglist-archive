{
    "numMessagesInTopic": 4, 
    "nextInTime": 7265, 
    "senderId": "AqYHnvv3WfyOPwusZd7ccw39CGwz4wDFP8KnfrCXtFV74QbFDhef54C-d_00p7xgu0w5Xe1XCikiQA3pq_Ig78D_Ms8aTldpN_6rqBcg5hgTe-SJpabI", 
    "systemMessage": false, 
    "subject": "Re: [junit] Is a regression testing framework available?", 
    "from": "&quot;Vladimir R. Bossicard&quot; &lt;vladimir@...&gt;", 
    "authorName": "Vladimir R. Bossicard", 
    "msgSnippet": "... Funny I discussed that with Bill Venners a few days ago. Your problem is that not all of your tests pass.  So my question is: how can a developer be sure", 
    "msgId": 7264, 
    "profile": "vbossica", 
    "topicId": 7256, 
    "spamInfo": {
        "reason": "0", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 43178095, 
    "messageBody": "<div id=\"ygrps-yiv-2062322502\"><blockquote><span title=\"ireply\">&gt; I would like to be able to find out which tests that have stopped <br/>\n&gt; working between two of our nightly builds (which run a set of JUnit <br/>\n&gt; tests). Since we currently have more than 500 test cases it is <br/>\n&gt; difficult to do this manually. It would of course be easy if the <br/>\n&gt; normal success rate was 100%, but a number of test cases fail since <br/>\n&gt; the implementation is not yet completed.<br/>\n<br/>\n </span></blockquote>Funny I discussed that with Bill Venners a few days ago.<br/>\n<br/>\nYour problem is that not all of your tests pass.  So my question is: how can a<br/>\ndeveloper be sure that he didn&#39;t introduce a bug with some new code?  X tests<br/>\nwere failing before the modification and now Y tests are failing.  If Y&gt;X you<br/>\ndon&#39;t know which one is/are now failing.<br/>\n<br/>\nIMO opinion (and in your case) you have two kinds of failing tests:<br/>\n- the one that shouldn&#39;t<br/>\n- the one that will, and it&#39;s ok (testing not implemented features, not<br/>\nimplemented tests)<br/>\n<br/>\nJUnit doesn&#39;t directly give you the possibility to separate the &quot;good&quot; from the<br/>\n&quot;bad&quot; failing tests.  You have to come up with your own solution.<br/>\n<br/>\nWhat I would do:<br/>\n- rename the tests that should fail testXYZ_failing<br/>\n- rewrite some JUnit components (it depends what kind of runner you&#39;re using<br/>\n(ant, swing, textui) to filter these tests<br/>\n- or write an XSLT file to process the XML and remove the &#39;_failing&#39; methods<br/>\nfrom the report<br/>\n<br/>\n-Vladimir<br/>\n<br/>\n-- <br/>\nVladimir R. Bossicard<br/>\nwww.bossicard.com</div>", 
    "prevInTime": 7263, 
    "specialLinks": [], 
    "contentTrasformed": false, 
    "postDate": "1048093016", 
    "canDelete": false, 
    "nextInTopic": 7270, 
    "prevInTopic": 7256, 
    "headers": {
        "inReplyToHeader": "PGI1OWZsdCttNzdoQGVHcm91cHMuY29tPg==", 
        "messageIdInHeader": "PDEwNDgwOTMwMTYuM2U3OGExNTgwY2UyY0BtYWlsLnNjZGkub3JnPg==", 
        "referencesHeader": "PGI1OWZsdCttNzdoQGVHcm91cHMuY29tPg=="
    }
}