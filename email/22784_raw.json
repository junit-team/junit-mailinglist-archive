{
    "numMessagesInTopic": 9, 
    "nextInTime": 22785, 
    "senderId": "fgXFMfHM1ZU66jKAsJM4TOJUdbAiZKTf8hoKVLuV89Q4OD3cZSEof0TkHmuRXAAxhPKK_Rpm2orUl9MzMCqn3HcEt_QuaJSTpFcuiA", 
    "systemMessage": false, 
    "subject": "Re: @BeforeClass method for DBUnit Dataset creation", 
    "from": "&quot;hdave321321&quot; &lt;hdave321321@...&gt;", 
    "authorName": "hdave321321", 
    "msgSnippet": "That is a compelling story -- thanks for taking the time to tell it. We use a best practice that each test class must have its own independent test data set.", 
    "msgId": 22784, 
    "rawEmail": "Return-Path: &lt;hdave321321@...&gt;\r\nReceived: (qmail 24307 invoked by uid 7800); 2 Jul 2010 15:47:52 -0000\r\nX-Sender: hdave321321@...\r\nX-Apparently-To: junit@yahoogroups.com\r\nX-Received: (qmail 57582 invoked from network); 2 Jul 2010 15:36:07 -0000\r\nX-Received: from unknown (98.137.34.44)\n  by m4.grp.sp2.yahoo.com with QMQP; 2 Jul 2010 15:36:07 -0000\r\nX-Received: from unknown (HELO n40b.bullet.mail.sp1.yahoo.com) (66.163.168.154)\n  by mta1.grp.sp2.yahoo.com with SMTP; 2 Jul 2010 15:36:07 -0000\r\nX-Received: from [69.147.65.148] by n40.bullet.mail.sp1.yahoo.com with NNFMP; 02 Jul 2010 15:32:45 -0000\r\nX-Received: from [98.137.35.12] by t11.bullet.mail.sp1.yahoo.com with NNFMP; 02 Jul 2010 15:32:45 -0000\r\nDate: Fri, 02 Jul 2010 15:32:44 -0000\r\nTo: junit@yahoogroups.com\r\nMessage-ID: &lt;i0l0qs+m77m@...&gt;\r\nIn-Reply-To: &lt;4C2D039F.4010605@...&gt;\r\nUser-Agent: eGroups-EW/0.82\r\nMIME-Version: 1.0\r\nContent-Type: text/plain; charset=&quot;ISO-8859-1&quot;\r\nContent-Transfer-Encoding: quoted-printable\r\nX-Mailer: Yahoo Groups Message Poster\r\nX-Yahoo-Newman-Property: groups-compose\r\nFrom: &quot;hdave321321&quot; &lt;hdave321321@...&gt;\r\nSubject: Re: @BeforeClass method for DBUnit Dataset creation\r\nX-Yahoo-Group-Post: member; u=453277943; y=id_EA3_DsvXjvhmV6U5lCLygQImQuwf1z9w3bcu4iioPvZL-viw\r\nX-Yahoo-Profile: hdave321321\r\nX-eGroups-Approved-By: dsaff &lt;david@...&gt; via email; 2 Jul 2010 15:47:52 -0000\r\n\r\nThat is a compelling story -- thanks for taking the time to tell it.\n\nWe us=\r\ne a best practice that each test class must have its own independent test d=\r\nata set.  Right now we have only 8 test class and 8 test data sets.  But so=\r\non we&#39;ll have 50+ test classes and so we&#39;ll have 50+ test data sets.  Recal=\r\nl that Spring Test performs a rollback after every test method, so there ar=\r\ne no dependencies between tests.\n\nIn your experience, is one data set per t=\r\nest *class* good enough?  It&#39;s unclear from your narrative if you are sugge=\r\nsting one data set per test *method* or just avoiding/minimizing integratio=\r\nn testing altogether in favor of unit testing.\n\nI have found these DAO inte=\r\ngration tests to be extremely helpful, given all the subtleties and magic s=\r\nurrounding Spring and Hibernate.\n\n\n\n \n\n--- In junit@yahoogroups.com, &quot;J. B.=\r\n Rainsberger&quot; &lt;jbrains762@...&gt; wrote:\n&gt;\n&gt; hdave321321 wrote:\n&gt; \n&gt; &gt; My DAO =\r\nintegration tests are taking a long time to run.  I am using\n&gt; &gt; Spring Tes=\r\nt which automatically does a transaction rollback after\n&gt; &gt; each test metho=\r\nd.  However, I am also using DBUnit to load the exact\n&gt; &gt; same dataset befo=\r\nre each test method via the @BeforeTransaction\n&gt; &gt; annotation.\n&gt; &gt;\n&gt; &gt; It h=\r\nas occured to me that I could save a lot of time if I load the\n&gt; &gt; DBUnit t=\r\nest data once per test class instead of once per test\n&gt; &gt; method.\n&gt; &gt;\n&gt; &gt; I=\r\n&#39;ve not used the @BeforeClass and @AfterClass methods before, but\n&gt; &gt; appar=\r\nently they require the annotated method to be static and this is\n&gt; &gt; interf=\r\nering with my normal use of Spring dependency injection that\n&gt; &gt; provides a=\r\nn application context, data source, and other things I need\n&gt; &gt; to load the=\r\n correct data set.\n&gt; &gt;\n&gt; &gt; Anyone experience this before?  What&#39;s a good wa=\r\ny to address this\n&gt; &gt; problem?\n&gt; \n&gt; Let me tell you a story.\n&gt; \n&gt; In 2001 I=\r\n worked on my first XP project at Toyota Canada in Toronto. We \n&gt; ran into =\r\nexactly the same problem you describe here: we had several \n&gt; dozen tests, =\r\nmost of which inserted data in setUp() and rolled \n&gt; transactions back in t=\r\nearDown(). Back then, of course, computers ran \n&gt; more slowly, so we were r=\r\nunning at around 5-10 tests per second when we \n&gt; were lucky. We could see =\r\nhow when we added another 300 tests our test \n&gt; suite would take several mi=\r\nnutes to run, and that just wasn&#39;t going to \n&gt; work. We decided to create a=\r\n single test data set for the entire test \n&gt; suite, then used the tools of =\r\nthe time--DBUnit, in fact--to insert the \n&gt; data set at the start of the te=\r\nst run. For a few weeks, it worked great. \n&gt; Our test suite execution time =\r\ndropped from minutes to tens of seconds \n&gt; which, back then, we could live =\r\nwith.\n&gt; \n&gt; Then something annoying happened.\n&gt; \n&gt; We added a new feature, w=\r\nhich required putting data in a new table, \n&gt; which itself required adding =\r\na row to an old table to set up a foreign \n&gt; key relationship. We got the s=\r\nix new tests to run, but when we ran the \n&gt; entire suite just before checki=\r\nn, 3 tests failed. We discovered that by \n&gt; adding a row to the parent tabl=\r\ne, we changed the expected result for \n&gt; three of our tests: they used to e=\r\nxpect 6 rows, and now they needed to \n&gt; expect 7. We considered two options=\r\n: split the test data into two sets, \n&gt; or change the expected results in t=\r\nhe tests. We figured that the first \n&gt; option would slowly lead us back to =\r\nloading test data for each test, so \n&gt; we chose the second option and chang=\r\ned the expected results.\n&gt; \n&gt; A couple of weeks later, it happened again. W=\r\ne added a feature that \n&gt; required putting data in a new table, which itsel=\r\nf required adding rows \n&gt; to a few different existing tables to set up fore=\r\nign key relationships. \n&gt; We got the eight new tests run, but when we ran t=\r\nhe whole suite, 10 \n&gt; tests failed. We sighed, we checked what happened, an=\r\nd we updated the \n&gt; expected results on the old tests.\n&gt; \n&gt; A week later, i=\r\nt happened again. This time we had to change 26 tests. It \n&gt; took a day to =\r\ndo it accurately, because each &quot;fix&quot; required running an \n&gt; 8-minute test s=\r\nuite, and 26 times 8 is already over three hours.\n&gt; \n&gt; A week later, it hap=\r\npened again. This time we had to change 61 tests. It \n&gt; took three days. We=\r\n saw where this was going.\n&gt; \n&gt; We split the test data back into sensible d=\r\natasets, with each test \n&gt; loading the data it needed. Execution time incre=\r\nased, but maintenance \n&gt; time went to almost zero. We also figured out how =\r\nto test more of the \n&gt; system without touching the database at all. Nowaday=\r\ns, when I work on \n&gt; such a project, only about 20-50 tests touch the datab=\r\nase, and the rest \n&gt; run in memory. At today&#39;s machine speeds, I usually ge=\r\nt 250-500 tests \n&gt; per second. In the ten seconds it takes for my mind to w=\r\nander, I can run \n&gt; 15,000-30,000 tests. I can build a pretty good system f=\r\nor 15,000 tests.\n&gt; -- \n&gt; J. B. Rainsberger :: http://www.jbrains.ca ::\n&gt; ht=\r\ntp://www.thecodewhisperer.com\n&gt;\n\n\n\n", 
    "profile": "hdave321321", 
    "topicId": 22776, 
    "spamInfo": {
        "reason": "0", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 453277943, 
    "prevInTime": 22783, 
    "contentTrasformed": false, 
    "postDate": "1278084764", 
    "canDelete": false, 
    "nextInTopic": 22791, 
    "prevInTopic": 22783, 
    "headers": {
        "inReplyToHeader": "PDRDMkQwMzlGLjQwMTA2MDVAZ21haWwuY29tPg==", 
        "messageIdInHeader": "PGkwbDBxcyttNzdtQGVHcm91cHMuY29tPg=="
    }
}