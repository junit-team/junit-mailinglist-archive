{
    "numMessagesInTopic": 8, 
    "nextInTime": 22402, 
    "senderId": "Z_0QUF3_fa3uPA29SdRt1PmY89pst0gDlw2fQB1if1t3LDVuMcNMCk9ZQSW_DMmsbS98Bh-ahYBM07XQzC2K-1xSOAbjPGXjhUmtBExR_-OzrDQoUNisvjVjvXC-C2s", 
    "systemMessage": false, 
    "subject": "Re: [junit] When to use parameterized tests vs. theories?", 
    "from": "=?UTF-8?B?Q8OpZHJpYyBCZXVzdCDimZQ=?= &lt;cbeust@...&gt;", 
    "authorName": "Cédric Beust ♔", 
    "msgSnippet": "If my understanding is correct, there is definitely an overlap between parameterized tests and theories. Theories generate a set of tuples representing the", 
    "msgId": 22401, 
    "profile": "cbeust", 
    "topicId": 22400, 
    "spamInfo": {
        "reason": "12", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 199443513, 
    "messageBody": "<div id=\"ygrps-yiv-1852700025\">If my understanding is correct, there is definitely an overlap between<br/>\nparameterized tests and theories.<br/>\n<br/>\nTheories generate a set of tuples representing the combination of a data<br/>\nset, while parameterized can return... well, any data set.  From that angle,<br/>\ntheories are a subset of parameterized tests, and as such, might have been<br/>\nbetter off implemented as a utility function rather than a full fledged<br/>\nfeature.<br/>\n<br/>\n-- <br/>\nCedric<br/>\n<br/>\n<br/>\n<br/>\nOn Fri, Feb 12, 2010 at 11:12 AM, Loritsch, Berin C. &lt;<br/>\n<a rel=\"nofollow\" target=\"_blank\" href=\"mailto:berin.loritsch@...\">berin.loritsch@...</a>&gt; wrote:<br/>\n<br/>\n<blockquote><span title=\"ireply\"> &gt;<br/>\n&gt;<br/>\n&gt; I know I&#39;m a little slow on the uptake, but I&#39;m asking these questions<br/>\n&gt; so that I can formulate how I&#39;m going to mentor my team to make the most<br/>\n&gt; of their testing investment.<br/>\n&gt;<br/>\n&gt; There is a certain amount of overlap between parameterized tests and<br/>\n&gt; theories. The parameterized tests are a bit more of a pain to set up;<br/>\n&gt; but they provide a little more control over how things get matched up.<br/>\n&gt; Of course, theories allow for much more robust data sets by combining<br/>\n&gt; all the data points in every way. This can provide an order of<br/>\n&gt; magnitude more tests automatically, but care has to be taken. For<br/>\n&gt; example, the number of actual tests run is D^P where D is the number of<br/>\n&gt; data points and P is the number of parameters on any given theory. This<br/>\n&gt; can generate a lot of tests quickly, and the &quot;runDiscretely&quot; option I<br/>\n&gt; hacked together for theories would definitely not be a preferable.<br/>\n&gt;<br/>\n&gt; I now know that assumptions are filtering devices for theories, so it<br/>\n&gt; does not mark the test as failed. Essentially, the reason that an<br/>\n&gt; assumption is marked as passing for the theory is that the assumption<br/>\n&gt; successfully filtered out that data combination. I get that now, but I<br/>\n&gt; probably will use that feature very sparingly.<br/>\n&gt;<br/>\n&gt; The question I have is what problem was the reason parameterized tests<br/>\n&gt; supposed to solve, and how is that different than the problem that<br/>\n&gt; theories are supposed to solve?<br/>\n&gt;<br/>\n&gt; [Non-text portions of this message have been removed]<br/>\n&gt;<br/>\n&gt;  <br/>\n&gt;<br/>\n<br/>\n<br/>\n<br/>\n </span></blockquote>-- <br/>\n***C�dric<br/>\n*<br/>\n<br/>\n<br/>\n[Non-text portions of this message have been removed]</div>", 
    "prevInTime": 22400, 
    "specialLinks": [], 
    "contentTrasformed": false, 
    "postDate": "1266008322", 
    "canDelete": false, 
    "nextInTopic": 22402, 
    "prevInTopic": 22400, 
    "headers": {
        "inReplyToHeader": "PDc1RjVFRDhBNUE1NTM2NDZBNDZFMDBFNkVDODU0RkEzMDQzRjk5MTJAdmFmZjAxLW1haWwwMS5hZC5nZC1haXMuY29tPg==", 
        "messageIdInHeader": "PGI4NmI2YTljMTAwMjEyMTI1OHkyZDIyYjg2dDc4YzJjM2ZjMzM1NWUzNmNAbWFpbC5nbWFpbC5jb20+", 
        "referencesHeader": "PDc1RjVFRDhBNUE1NTM2NDZBNDZFMDBFNkVDODU0RkEzMDQzRjk5MTJAdmFmZjAxLW1haWwwMS5hZC5nZC1haXMuY29tPg=="
    }
}