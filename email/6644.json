{
    "numMessagesInTopic": 2, 
    "nextInTime": 6645, 
    "senderId": "MxsQOFMmGZIU3hp5G29d8IRlkI4zqIh5rtf5CynrtvaRB9A4YBLTZzhWc6dSX-_6LmkqDgV5j9DlEiS3TxpGkHMv", 
    "systemMessage": false, 
    "subject": "Re: [junit] Digest Number 739", 
    "from": "steffen.gemkow@...", 
    "authorName": "steffen.gemkow@objectfab.de", 
    "msgSnippet": "Hi, as one of the autors of a test generator, I d like to respond to the question frome Steve and a few of the answers from people in this group. Steve asked", 
    "msgId": 6644, 
    "profile": "s_gemkow", 
    "topicId": 6644, 
    "spamInfo": {
        "reason": "0", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 127226929, 
    "messageBody": "<div id=\"ygrps-yiv-45167057\">Hi, <br/>\n<br/>\nas one of the autors of a test generator, I&#39;d like to respond to the question frome Steve and a few of the answers from people in this<br/>\ngroup.<br/>\n<br/>\nSteve asked with regard to empty methods generated by <br/>\ntest generators:<br/>\n<br/>\n<blockquote><span title=\"ireply\"> &gt; &gt; Is there a way to track the number of assertions that pass/fail<br/>\n&gt; &gt; instead of the number of methods that are invoked in the test classes?<br/>\n&gt; &gt;  This way the numbers reported (e.g., 122 tests run) would tend to<br/>\n&gt; &gt; imply better test coverage, or at least more actual tests executing,<br/>\n&gt; &gt; rather than lots of empty tests running.  (Yes, I know bad tests can<br/>\n<br/>\n<br/>\n&gt; I&#39;m tempted to say that the generated tests is what you should look <br/>\n&gt; into, rather than JUnit.  For example, if your generated test put <br/>\n&gt; something like this into the body:<br/>\n&gt; <br/>\n&gt; throw new Exception(&quot;Test Not Yet Implemented&quot;);<br/>\n&gt; <br/>\n&gt; -shane<br/>\n <br/>\n<br/>\n&gt;    From: &quot;J. B. Rainsberger&quot; &lt;<a rel=\"nofollow\" target=\"_blank\" href=\"mailto:jbr@...\">jbr@...</a>&gt;<br/>\n&gt; Then fix the test generator. I use an explicit failure to indicate that<br/>\n&gt; I have not yet written the test. In fact, in my &quot;JUnit: A Starter<br/>\n&gt; Guide&quot;, I suggest starting with this test:<br/>\n&gt; <br/>\n&gt; testSomething:<br/>\n&gt;     fail(&quot;Not yet implemented&quot;)<br/>\n&gt; <br/>\n&gt; As I add code to the test, I keep the fail statement in there until I<br/>\n&gt; have decided that the test is complete.<br/>\n&gt; <br/>\n&gt; It sounds to me like you should consider changing the test generator,<br/>\n&gt; and not JUnit, to solve your problem.<br/>\n<br/>\n </span></blockquote>Why are there empty methods at all?<br/>\n <br/>\nI know, that some of you practice TDD and are very active in the <br/>\ntestdrivendevelopment list. Empty tests won&#39;t happen with TDD, <br/>\nat least not without any generator. ;-)<br/>\n<br/>\nIf you have at least a compilable skeleton of your application class,<br/>\ntools like our JUnitDoclet or many others can generate a skeleton <br/>\nof TestCases - just the skeleton, not much flesh. So I&#39;d like to<br/>\nconsider the generating as:<br/>\n  Offering a place to fill in some tests very easy. <br/>\n(You may nee functional tests too, but that&#39;s of topic here.)<br/>\nAny code generator will do that, after all that is what they are <br/>\nbuild for. If you don&#39;t fill in the tests, that is a different thing.<br/>\n<br/>\nA generator could put fail(); in any empty tests very easy.<br/>\nBut there is a major downside too, especially for regenerating <br/>\ntools loke JUnitDoclet. I don&#39;t want to copy/paste half <br/>\na page to this email, please see:<br/>\n  <a rel=\"nofollow\" target=\"_blank\" href=\"http://www.junitdoclet.org/faq.html#fail_not_default\">http://www.junitdoclet.org/faq.html#fail_not_default</a> <br/>\n<br/>\nBack to the question from Steve. <br/>\n<br/>\nHe asked me the same thing directly as well. If I understand <br/>\nhim right, he want&#39;s to measure his progress in writing tests <br/>\nfor a given application. (With a generator, the number of <br/>\ntests is a constant as long as the application stays as it is.)<br/>\n<br/>\n * Counting the assertions is one option he came up with. <br/>\n    <br/>\n * I suggested the size of all tests as a very rough estimate<br/>\n   to be another option. (Since I&#39;m not payed by kbyte, <br/>\n   I prefer that way of measuring progress because of it&#39;s <br/>\n   simplicity.)<br/>\n<br/>\n * Test coverage tools are very likely the most precice option.<br/>\n<br/>\n * Removing empty methods as an additional step in building<br/>\n   the tests could serve as an option too. (A simple script could <br/>\n   do that.) But the estimate is not much better than &quot;size of<br/>\n   all tests&quot;, isn&#39;t it? (You meight measure the number of tests <br/>\n   you&#39;ve touched, but not the care you put into each test.)<br/>\n<br/>\nAnyway, the next version of JUnitDoclet will report the number <br/>\nof empty methods each time it generates. Maybe we provide <br/>\nan option to list them too.  <br/>\n<br/>\nI hope, this is was helpful to someone.<br/>\n<br/>\nKind Regards,<br/>\n<br/>\nSteffen Gemkow</div>", 
    "prevInTime": 6643, 
    "specialLinks": [], 
    "contentTrasformed": false, 
    "postDate": "1042283544", 
    "canDelete": false, 
    "nextInTopic": 6655, 
    "prevInTopic": 0, 
    "headers": {
        "inReplyToHeader": "PDEwNDIyNzQwMzcuNDY5LjQzMjU3Lm0xMkB5YWhvb2dyb3Vwcy5jb20+", 
        "messageIdInHeader": "PDNFMjAwQTI4LjEzMjQ5Ljc2NUM4RkBsb2NhbGhvc3Q+"
    }
}