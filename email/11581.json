{
    "numMessagesInTopic": 28, 
    "nextInTime": 11582, 
    "senderId": "rxx8HMTb-ef9XxPYbjPUEq-_k4SotyQFAABcOafNeFNJkilI4G9twSOrVrqGwaRDahKfGZvLSaFqvD9I4Rvl2nAN-fn7sOsDpQ", 
    "systemMessage": false, 
    "subject": "Re: [junit] a test framework to codify levelizaton?", 
    "from": "Vlad Roubtsov &lt;vladrimp@...&gt;", 
    "authorName": "Vlad Roubtsov", 
    "msgSnippet": "I ve waited to get some data from others before replying. It appears that what I had in mind were testcases more functional in nature. In my experience on a", 
    "msgId": 11581, 
    "profile": "vlad_r333", 
    "topicId": 11558, 
    "spamInfo": {
        "reason": "0", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 130143637, 
    "messageBody": "<div id=\"ygrps-yiv-381575932\">I&#39;ve waited to get some data from others before<br/>\nreplying. It appears that what I had in mind were<br/>\ntestcases more functional in nature. In my experience<br/>\non a ~1GHz desktop a testsuite with 100-200 testcases<br/>\nwill take from 2 to 5 minutes. This translates into<br/>\n1-2 seconds per testcase.<br/>\n<br/>\nThe cost usually comes from a certain mix of CPU and<br/>\nJDBC utilization. The company I work for provides most<br/>\nof its high-value software via &quot;engines&quot; that operate<br/>\non complex &quot;models&quot;. The models are compiled from db<br/>\ndata or XML input and to get any kind of &quot;data/path<br/>\ncoverage&quot; in the engines/models they need to be<br/>\nexercised with a large number of different inputs. <br/>\n<br/>\n5 minutes seems to be about the threshold where team<br/>\nmembers can&#39;t help it and start cheating by removing<br/>\nsome testcases from their own dev runs. This sometimes<br/>\ntranslates into checkin errors because parts of the<br/>\ntestsuite develop failures not noticed until later.<br/>\n<br/>\nRegarding the actual method for inferring<br/>\ndependencies, I had a couple of ideas:<br/>\n<br/>\n(a) rely on explict markup from the developer(s).<br/>\nJavadoc pseudo-tags and real J2SE 1.5 metadata tags.<br/>\nThis has the advantage of being at a higher level of<br/>\nabstraction than mere .class dependencies if<br/>\nnecessary. However, if tagging is in the hands of more<br/>\nthan one architect on the team, some sort of an<br/>\nagreement on tag semantics/metadata would need to be<br/>\nestablished and that seems like a weakness.<br/>\n<br/>\n(b) go the low level route of analyzing .class<br/>\ndependencies. I wouldn&#39;t use reflection for this (that<br/>\ndata is insufficient) but rather rely on .class<br/>\ncontent for computing dependencies. I happen to have<br/>\nsome direct experience in this area and this can be<br/>\ndone very efficiently (sub-1ms per class). If I were<br/>\nto imagine a test suite runner that starts by scanning<br/>\nthe classpath to compute levelization this way, I<br/>\ndon&#39;t think there would be a delay more than ~1-2<br/>\nseconds per every 1000 classes, which is quite<br/>\nacceptable.<br/>\n<br/>\nOf course, the second option hits the usual problem of<br/>\nnot knowing about reflective and<br/>\nClass.forName()invocations... Anyway, I was wondering<br/>\nif there was a project tackling this somewhere.<br/>\n<br/>\nVlad.<br/>\n<br/>\n--- &quot;J. B. Rainsberger&quot; &lt;<a rel=\"nofollow\" target=\"_blank\" href=\"mailto:jbrains@...\">jbrains@...</a>&gt; wrote:<br/>\n<blockquote><span title=\"ireply\"> &gt; Vlad Roubtsov wrote:<br/>\n&gt; <br/>\n&gt; &gt; On any but the trivial projects the number of<br/>\n&gt; &gt; testcases quickly exceeds 100-200+. It usually<br/>\n&gt; takes a<br/>\n&gt; &gt; noticeable time to run them all.<br/>\n&gt; <br/>\n&gt; Can you quantify this? GSBase, for example, runs<br/>\n&gt; tests at approximately <br/>\n&gt; 100/second, and that&#39;s my usual performance goal for<br/>\n&gt; tests. On my most <br/>\n&gt; recent project, I separated tests that required<br/>\n&gt; expensive, external <br/>\n&gt; resources (there were about 10) from tests than ran<br/>\n&gt; entirely in memory <br/>\n&gt; (there were about 400). The in-memory tests ran in<br/>\n&gt; about 6 seconds.<br/>\n&gt; <br/>\n&gt; Is this &quot;noticeable&quot; by your standards? I&#39;d merely<br/>\n&gt; like some extra context.<br/>\n&gt; <br/>\n&gt;  &gt; It is also less<br/>\n&gt; &gt; efficient to do so because it ignores the idea of<br/>\n&gt; &gt; levelization (from John Lakos&#39; underappreciated<br/>\n&gt; book<br/>\n&gt; &gt; &quot;Large Scale C++ Software Design&quot;). In incremental<br/>\n&gt; dev<br/>\n&gt; &gt; mode it would be more efficient to execute only<br/>\n&gt; the<br/>\n&gt; &gt; testcases that correspond to the product<br/>\n&gt; package(s)<br/>\n&gt; &gt; that&#39;s just changed (and the dependency closure<br/>\n&gt; &gt; thereof) AND execute them in increasing level<br/>\n&gt; numbers<br/>\n&gt; &gt; (reverse package dependency topological order).<br/>\n&gt; <br/>\n&gt; This is a great idea, however, computing the<br/>\n&gt; complete dependency closure <br/>\n&gt; is awfully tricky. For most algorithms that attempt<br/>\n&gt; to do this, I can <br/>\n&gt; load a class through reflection and use it, breaking<br/>\n&gt; the algorithm&#39;s <br/>\n&gt; assertion that it correctly computes the runtime<br/>\n&gt; dependency closure. I&#39;m <br/>\n&gt; not saying it&#39;s impossible -- although it might be<br/>\n&gt; -- but I am doubtful <br/>\n&gt; that any existing tool does this adequately and<br/>\n&gt; could be integrated into <br/>\n&gt; Eclipse, for example.<br/>\n&gt; <br/>\n&gt; &gt; A tool/framework that enforces test (sub)suite<br/>\n&gt; &gt; execution in increasing level numbers would seem<br/>\n&gt; like<br/>\n&gt; &gt; the next evolutionary step after naive TDD. Does<br/>\n&gt; &gt; anyone know of such a tool? The testcase-&gt;app<br/>\n&gt; package<br/>\n&gt; &gt; assignment could be done by simply placing<br/>\n&gt; testcases<br/>\n&gt; &gt; in the relevant package (common practice already<br/>\n&gt; &gt; anyway) or by something like Java 1.5 metadata<br/>\n&gt; &gt; tagging.<br/>\n&gt; <br/>\n&gt; I&#39;m not aware of anything, but if it exists, the<br/>\n&gt; C#/.NET world would be <br/>\n&gt; a likely first place to look. They&#39;ve had metadata<br/>\n&gt; tagging longer than <br/>\n&gt; Java folks have.<br/>\n&gt; <br/>\n&gt; &gt; There are tools to analyze existing package<br/>\n&gt; &gt; dependencies, but I am not aware of a test<br/>\n&gt; framework<br/>\n&gt; &gt; to actually relate that data to testcase execution<br/>\n&gt; &gt; ordering.<br/>\n&gt; <br/>\n&gt; Those tools likely only analyze compile-time<br/>\n&gt; (structural) dependecies, <br/>\n&gt; and as I wrote above, it would be all too easy to<br/>\n&gt; exhibit a runtime <br/>\n&gt; dependency that such an analyzer would miss. The<br/>\n&gt; result is that runtime <br/>\n&gt; behavior might change without the analyzer knowing<br/>\n&gt; to run all the <br/>\n&gt; appropriate tests.<br/>\n&gt; <br/>\n&gt; I am admittedly shaky on the theory, and welcome<br/>\n&gt; being proven wrong, but <br/>\n&gt; I have this vague notion that I&#39;m right.<br/>\n&gt; -- <br/>\n&gt; J. B. Rainsberger,<br/>\n<br/>\n<br/>\n\t\t<br/>\n </span></blockquote>__________________________________<br/>\nDo you Yahoo!?<br/>\nVote for the stars of Yahoo!&#39;s next ad campaign!<br/>\n<a rel=\"nofollow\" target=\"_blank\" href=\"http://advision.webevents.yahoo.com/yahoo/votelifeengine/\">http://advision.webevents.yahoo.com/yahoo/votelifeengine/</a></div>", 
    "prevInTime": 11580, 
    "specialLinks": [], 
    "contentTrasformed": false, 
    "postDate": "1090245691", 
    "canDelete": false, 
    "nextInTopic": 11583, 
    "prevInTopic": 11578, 
    "headers": {
        "inReplyToHeader": "PDQwRjhBNEY3LjgwOTA1MDZAcm9nZXJzLmNvbT4=", 
        "messageIdInHeader": "PDIwMDQwNzE5MTQwMTMxLjk2MTAzLnFtYWlsQHdlYjUwMzA0Lm1haWwueWFob28uY29tPg=="
    }
}