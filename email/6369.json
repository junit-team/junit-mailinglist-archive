{
    "numMessagesInTopic": 18, 
    "nextInTime": 6370, 
    "senderId": "vdGKbWbOBpW0U9sy1WhlUrTrk-5iiZyV9-b04sAmW3ElPoY8bBP5MKOVzrxtWaagnqRTYE0UOn9CQBwSsHyhtylPjZYcoKPGvMeQY4IUX1ySiBSn2TE", 
    "systemMessage": false, 
    "subject": "Re: [junit] Running time consuming JUnit Tests", 
    "from": "&quot;Vladimir R. Bossicard&quot; &lt;vbossica@...&gt;", 
    "authorName": "Vladimir R. Bossicard", 
    "msgSnippet": "... -- Vladimir R. Bossicard www.bossicard.com", 
    "msgId": 6369, 
    "profile": "vbossica", 
    "topicId": 6359, 
    "spamInfo": {
        "reason": "0", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 112965394, 
    "messageBody": "<div id=\"ygrps-yiv-125355583\"><blockquote><span title=\"ireply\">&gt; This has to do with understanding the role of programmer tests as<br/>\n&gt; opposed to customer tests. I have some fuzzy ideas in this direction,<br/>\n&gt; but nothing to make a paper possible yet. The bottom line is<br/>\n&gt; understanding the importance of blazing fast programmer tests and of<br/>\n&gt; separating PTs from CTs.<br/>\n<br/>\n </span></blockquote>What I did for Xindice (XML Database) is to separate the tests (unit,<br/>\nintegration and stress).  The unit tests run all the time but the integration<br/>\ntests run less often.<br/>\n<br/>\nThe problem is that Xindice provides several drivers (xml-rpc and embed) to<br/>\naccess the database.  To test the xml-rpc driver, you have to generate the<br/>\nxindice.war file, copy it into your Tomcat&#39;s (e.g.) webapps directory and<br/>\nreload it.  If you do this each time you make a tiny modification it&#39;s quite<br/>\npainful (and not everyone has a Tomcat server running).  So it was natural to<br/>\nseparate the tests according to their complexity.<br/>\n<br/>\nif you&#39;re interested, you can browse the code<br/>\n&lt;<a rel=\"nofollow\" target=\"_blank\" href=\"http://cvs.apache.org/viewcvs.cgi/xml-xindice/java/tests/\">http://cvs.apache.org/viewcvs.cgi/xml-xindice/java/tests/</a>&gt;<br/>\n<br/>\nAnother question: how would you handle a test that is supposed to pass but that<br/>\n currently fails because of a problem you can&#39;t fix?  &quot;All the tests must pass<br/>\nall the time&quot; is fine if you have control over the entire code, but what if one<br/>\nof the libraries you use has a bug?  While other are fixing the library, your<br/>\ntests are still broken but it&#39;s still ok to commit new code (because no other<br/>\nnew test fail)...<br/>\n<br/>\nHow can you handle the situation?  Removing the failing test (altering its name<br/>\nfor example) is IMO not so clean if you don&#39;t have a mechanism to automatically<br/>\nspot them.<br/>\n<br/>\n-Vladimir<br/>\n\r<br/>\n--<br/>\nVladimir R. Bossicard<br/>\nwww.bossicard.com</div>", 
    "prevInTime": 6368, 
    "specialLinks": [], 
    "contentTrasformed": false, 
    "postDate": "1039166564", 
    "canDelete": false, 
    "nextInTopic": 6370, 
    "prevInTopic": 6368, 
    "headers": {
        "messageIdInHeader": "PDIwMDIxMjA2MDEyMjQ2LjI2MDk1LmgwMjEuYzAwMy53bUBtYWlsLnRlbG9jaXR5LmNvbS5jcml0aWNhbHBhdGgubmV0Pg=="
    }
}