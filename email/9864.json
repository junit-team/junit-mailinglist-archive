{
    "numMessagesInTopic": 18, 
    "nextInTime": 9865, 
    "senderId": "fUxvjPClRIWS_z6lfzB5pCH5b-T4PbfnpB-_BVFsDKy6RO93YmNhomivCUgS85cSEozLw1MycA7R-_7GzBhYsVeYteCZD81SXG4biHg", 
    "systemMessage": false, 
    "subject": "Re: performance testing", 
    "from": "Chad Woolley &lt;lists@...&gt;", 
    "authorName": "Chad Woolley", 
    "msgSnippet": "This is a situation where Aspect-Oriented Programming might be useful after all. This might be caused because the framework is doing some Other Stuff which", 
    "msgId": 9864, 
    "profile": "thewoolleyman", 
    "topicId": 9829, 
    "spamInfo": {
        "reason": "0", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 142650165, 
    "messageBody": "<div id=\"ygrps-yiv-887204175\">This is a situation where Aspect-Oriented Programming might be useful <br/>\nafter all.<br/>\n<br/>\nThis might be caused because the framework is doing some &quot;Other Stuff&quot; <br/>\nwhich is getting in the way of timing the actual execution time of the <br/>\nreal work.  It may not be easy to extend the framework to get your <br/>\nnumbers and still get &quot;close enough&quot; to the actual test execution to get <br/>\nmeaningful numbers.<br/>\n<br/>\nIt might turn out that the test fixture setup is obscuring the actual <br/>\nexecution time of the tests.  FOr example, if the fixture setup takes <br/>\nmuch longer than the actual execution of the Class Under Test.  This <br/>\nwould make it difficult to get meaningful numbers, since the variance in <br/>\nthe fixture execution time would obscure the actual test performance <br/>\nnumbers.<br/>\n<br/>\nIn this situation, you could come up with some scheme to wrap the <br/>\nmonitoring around the actual call to your Class Under Test, and avoid <br/>\nincluding your fixture setup in the monitoring.<br/>\n<br/>\nAOP could be well suited for this type of task, because otherwise you <br/>\nwould have to duplicate this monitoring code (or at least remember to <br/>\nduplicate a call to it) in each test method.<br/>\n<br/>\nThen again, there could be a way to accomplish the same thing with Plain <br/>\nOld Java.  Maybe by routing all calls to the Class Under Test through a <br/>\nmonitoring method.  This could take the class name, method name, and any <br/>\nparams, then use some fancy reflection to invoke the method and gather <br/>\nthe elapsed time.<br/>\n<br/>\n-- Chad<br/>\n<br/>\nJim Dixon wrote:<br/>\n<blockquote><span title=\"qreply\"> &gt; I haven&#39;t done very careful testing, but I certainly have noticed that<br/>\n&gt; the times reported by JUnit vary a great deal.<br/>\n&gt; <br/>\n&gt; Running one simple set of tests just now gave me results in which 50%<br/>\n&gt; differences were common and more than one test took 3x as long from one<br/>\n&gt; run to the next.  So, sure, you can use JUnit for this sort of simple<br/>\n&gt; performance testing, but you should at least run the tests enough times to<br/>\n&gt; build statistical confidence in the results.<br/>\n&gt; <br/>\n&gt; It didn&#39;t show up in these few runs, but I believe that I have noticed<br/>\n&gt; before that the first test reported a much longer than normal time --<br/>\n&gt; seconds rather than milliseconds.  So the order in which tests are run<br/>\n&gt; might introduce an effect large enough to render the results useless. </span></blockquote></div>", 
    "prevInTime": 9863, 
    "specialLinks": [], 
    "contentTrasformed": false, 
    "postDate": "1068747237", 
    "canDelete": false, 
    "nextInTopic": 9865, 
    "prevInTopic": 9862, 
    "headers": {
        "inReplyToHeader": "PDEwNjg3MzE0MTAuMTg2Ni4yNzAyNC5tMTJAeWFob29ncm91cHMuY29tPg==", 
        "messageIdInHeader": "PDNGQjNDOUU1LjkwNDA0MDRAdGhld29vbGxleXdlYi5jb20+", 
        "referencesHeader": "PDEwNjg3MzE0MTAuMTg2Ni4yNzAyNC5tMTJAeWFob29ncm91cHMuY29tPg=="
    }
}