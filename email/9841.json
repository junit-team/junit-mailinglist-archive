{
    "numMessagesInTopic": 18, 
    "nextInTime": 9842, 
    "senderId": "Hhv_qCahEMGIOubhkHbf5wqCWak4cInN6mcekGfuDwA89ogg0o9kvubouFb5zyMixMOOLcprjv4_OUUkcpObStIyGtNTf4ayo4Ko7A", 
    "systemMessage": false, 
    "subject": "RE: [junit] performance testing", 
    "from": "&quot;Morten Grum, PH-Consult&quot; &lt;mg@...&gt;", 
    "authorName": "Morten Grum, PH-Consult", 
    "msgSnippet": "Hi ... As far as I have observed the JUnit times are true run time and not CPU time. So they depend very much on all the other tasks that the machine happens", 
    "msgId": 9841, 
    "profile": "morten3grum", 
    "topicId": 9829, 
    "spamInfo": {
        "reason": "0", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 11862826, 
    "messageBody": "<div id=\"ygrps-yiv-2101476163\">Hi<br/>\n<br/>\n<blockquote><span title=\"ireply\"> &gt; On Tue, 11 Nov 2003, James Cheesman wrote:<br/>\n&gt;<br/>\n&gt; &gt; Doesn&#39;t JUnit do this already? At least if I run the tests with ant I<br/>\n&gt; &gt; get an output similar to:<br/>\n&gt; &gt;<br/>\n&gt; &gt; Testcase: testDateIsActiveNullDate took 0,016 sec<br/>\n&gt; &gt; Testcase: testDateIsActiveToday took 0,031 sec<br/>\n&gt; &gt; Testcase: testDateIsActivePast took 0,047 sec<br/>\n&gt; &gt; ...<br/>\n&gt; &gt;<br/>\n&gt; &gt; I&#39;m not sure how much use this kind of micro-testing would be, though<br/>\n&gt; &gt; I&#39;m sure you could set up your test cases to provide more valid data.<br/>\n&gt;<br/>\n&gt; I haven&#39;t done very careful testing, but I certainly have noticed that<br/>\n&gt; the times reported by JUnit vary a great deal.<br/>\n&gt;<br/>\n&gt; Running one simple set of tests just now gave me results in which 50%<br/>\n&gt; differences were common and more than one test took 3x as long from one<br/>\n&gt; run to the next.  So, sure, you can use JUnit for this sort of simple<br/>\n&gt; performance testing, but you should at least run the tests enough times to<br/>\n&gt; build statistical confidence in the results.<br/>\n&gt;<br/>\n&gt; It didn&#39;t show up in these few runs, but I believe that I have noticed<br/>\n&gt; before that the first test reported a much longer than normal time --<br/>\n&gt; seconds rather than milliseconds.  So the order in which tests are run<br/>\n&gt; might introduce an effect large enough to render the results useless.<br/>\n<br/>\n </span></blockquote>As far as I have observed the JUnit times are true run time and not CPU<br/>\ntime. So they depend very much on all the other tasks that the machine<br/>\nhappens to be running. They&#39;re a good first indicator but cannot really be<br/>\nused for evaluating the impact of a performance improvements.<br/>\n<br/>\nAnother thing is that compared to other languages, evaluating performance<br/>\nimprovements in Java requires that code is run in the same order each time<br/>\n(for a single start of the VM). Java simply runs familiar stuff faster than<br/>\nnew code.<br/>\n<br/>\nAlthough the test timings in JUnit may have little use in terms of<br/>\nperformance testing they have one very important and indispencable value: if<br/>\nrunning my &quot;AllTests&quot; takes a day or two I won&#39;t run them often enough, they<br/>\nmust take less than half an hour so it can run during lunch. The JUnit<br/>\ntimings guide me to where I should optimize my tests in order to maximize<br/>\n&quot;test value/test excecution time&quot;.<br/>\n<br/>\nMorten</div>", 
    "prevInTime": 9840, 
    "specialLinks": [], 
    "contentTrasformed": false, 
    "postDate": "1068711394", 
    "canDelete": false, 
    "nextInTopic": 9847, 
    "prevInTopic": 9840, 
    "headers": {
        "inReplyToHeader": "PDIwMDMxMTEyMTc0MDMzLkY2ODMzMi0xMDAwMDBAbG9jYWxob3N0Pg==", 
        "messageIdInHeader": "PEZFRUFJRklNTEhMS0FKQ0VQT05IQ0VNSUNKQUEubWdAcGhjLmRrPg=="
    }
}