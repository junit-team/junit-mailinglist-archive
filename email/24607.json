{
    "numMessagesInTopic": 6, 
    "nextInTime": 24608, 
    "senderId": "kLUwkuz4T5Q7fuFUuK2IhVIIZ0Ar9gdzvuMVLg_K8XlkWxu0_UJ4D3Y3CsV1mokSIFzu4Us68vmgZVf6YiYctXaEp8oc4N6jyvysKrNb", 
    "systemMessage": false, 
    "subject": "Re: [junit] Quo Vadis JUnit", 
    "from": "Esko Luontola &lt;esko.luontola@...&gt;", 
    "authorName": "Esko Luontola", 
    "msgSnippet": "I ll go over the topics on by one, adding some more related topics. For some of them I ve implemented a solution in the Jumi test runner (http://jumi.fi/) in", 
    "msgId": 24607, 
    "profile": "egeluontola", 
    "topicId": 24605, 
    "spamInfo": {
        "reason": "0", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 395435659, 
    "messageBody": "<div id=\"ygrps-yiv-235053427\">I&#39;ll go over the topics on by one, adding some more related topics. For <br/>\nsome of them I&#39;ve implemented a solution in the Jumi test runner <br/>\n(<a rel=\"nofollow\" target=\"_blank\" href=\"http://jumi.fi/)\">http://jumi.fi/)</a> in which case I will point out how I did it there.<br/>\n<br/>\n<br/>\n## Test names ##<br/>\n<br/>\nI think test names should allow any characters, including space and <br/>\npunctuation, so that they could be written as sentences describing the <br/>\ndesired features. At most newline may be disallowed.<br/>\n<br/>\n<br/>\n## Test identification ##<br/>\n<br/>\nA concern related to test names is test identification. In Jumi test <br/>\nnames do not have to be unique. For test identification it uses <br/>\nhierarchial numeric IDs <br/>\n(<a rel=\"nofollow\" target=\"_blank\" href=\"https://github.com/orfjackal/jumi/blob/master/jumi-api/src/main/java/fi/jumi/api/drivers/TestId.java)\">https://github.com/orfjackal/jumi/blob/master/jumi-api/src/main/java/fi/jumi/api/drivers/TestId.java)</a> <br/>\nwhich also signify the order in which the tests should be shown in test <br/>\nresults as a tree. Those IDs are not shown to the user and since the IDs <br/>\nmay change between test runs (e.g. if reflection returns test methods in <br/>\ndifferent order), using them for test filtering might not work, but <br/>\nfiltering would work better based on test names.<br/>\n<br/>\n<br/>\n## Multiple execution/cascading names ##<br/>\n<br/>\nIn Jumi tests may be executed multiple times and a test&#39;s execution may <br/>\ncontain the execution of other tests as sub-steps, in an arbitrarily <br/>\nnested structure. This supports all the &quot;BDD frameworks&quot; which often use <br/>\nnesting for organizing tests. See <br/>\n<a rel=\"nofollow\" target=\"_blank\" href=\"https://github.com/orfjackal/jumi/wiki/Execution-Model-and-Terminology\">https://github.com/orfjackal/jumi/wiki/Execution-Model-and-Terminology</a><br/>\n<br/>\n<br/>\n## Global Rules ##<br/>\n<br/>\nI don&#39;t know how this could be fully solved without tying it to the <br/>\ndetails of a particular testing framework. Even with JUnit 4 each <br/>\norg.junit.runner.Runner implementation is responsible for handling the <br/>\nrules itself. It may be hard/impossible to create global rules that wrap <br/>\nthe individual test methods of any testing frameworks.<br/>\n<br/>\nAs a limited solution, rules at the level of the whole suite or <br/>\nindividual test classes should be possible. For example a rule which <br/>\nstarts up the application/databases once before all tests and shuts it <br/>\ndown after all the tests are finished. Or a rule that wraps around the <br/>\nexecution of a test class, for example to set a timeout.<br/>\n<br/>\nI wonder how it should be declared that which global rules to use. <br/>\nProbably explicitly listing them in a configuration would be best. <br/>\nClasspath scanning would make it too unpredictable and it would delay <br/>\nthe first test&#39;s execution until after the scanning is finished.<br/>\n<br/>\n<br/>\n## Finding Tests ##<br/>\n<br/>\nAgreed, there should be a common way for doing this to avoid minor <br/>\ndifferences between tools. That&#39;s why also in Jumi I&#39;ve implemented test <br/>\ndiscovery as a built-in feature. It crawls though the compile output <br/>\ndirectories and looks for files which match a regex or glob pattern. <br/>\nTest execution happens in parallel with test discovery, starting <br/>\nimmediately when the first test class is found.<br/>\n<br/>\nUsing naming patterns has the added benefit that it&#39;s not tied to .class <br/>\nfiles, but may be used by frameworks which store tests in text files <br/>\n(e.g. Cucumber and other BDD frameworks) or in other programming <br/>\nlanguages (e.g. Clojure).<br/>\n<br/>\n<br/>\n## Finding Tests Incrementally ##<br/>\n<br/>\nIn some testing frameworks where test are declared as method calls (by <br/>\npassing in the test name and a closure), especially with nested tests, <br/>\nit&#39;s not possible to find out a list of all tests before executing any <br/>\ntests. This is one of the reasons why I created the Jumi test runner, <br/>\nbecause in the Specsy testing framework (<a rel=\"nofollow\" target=\"_blank\" href=\"http://specsy.org/)\">http://specsy.org/)</a> I had to <br/>\nuse hacks to work around the JUnit 4 test runner&#39;s assumption of knowing <br/>\nwhat tests exists before executing them (otherwise some IDEs behave <br/>\nweirdly).<br/>\n<br/>\n<br/>\n## Jump to source code ##<br/>\n<br/>\nSince there are lots of different ways of declaring tests, currently <br/>\nIDEs need to have custom support for each testing framework. I&#39;ve been <br/>\nthinking that it might be possible for a testing framework to report to <br/>\nthe test runner the location of each test (i.e. file and line number), <br/>\nso that IDEs would not need to figure it out themselves.<br/>\n<br/>\nFor JUnit 4 style testing frameworks, this could be done by reading line <br/>\nnumbers from the bytecode of the test methods. For testing frameworks <br/>\nwhere tests are declared by passing the test name and a closure as <br/>\narguments to a method, this could be done by creating an Exception and <br/>\nreading the line number of the calling method from the stack trace elements.<br/>\n<br/>\n<br/>\n## Filtering ##<br/>\n<br/>\nRelated to the above discussion about test identification, I think using <br/>\ntest names is the most reliable method of filtering tests, with the <br/>\n(minor) caveat that running a single method may not be possible if two <br/>\ntests have the same name in the same class.<br/>\n<br/>\nRelated to the above discussion about cascading and nested tests, I <br/>\nthink that test filtering will need to take into account the test <br/>\nhierarchy. So instead of just a single filtering pattern (regex?), there <br/>\nwould need to be a list of them, one for each depth level of tests.<br/>\n<br/>\nI have some doubts whether it will be possible to create a filtering <br/>\nmechanism that fits every testing framework. Some frameworks may be able <br/>\nto support the filtering mechanism only partly. Also IDE support may <br/>\nhave to vary between testing frameworks - in the general case the IDE <br/>\nwill know what tests exists only after the tests have been run once, <br/>\ni.e. it would not be possible to select inside the editor &quot;run only this <br/>\ntest&quot; unless the IDE has custom support for the testing framework in <br/>\nquestion.<br/>\n<br/>\n<br/>\n## Test Groups ##<br/>\n<br/>\nRelated to filtering and finding tests is test groups/categories. For <br/>\nexample we want to make it easy to run just unit tests without <br/>\nintegration tests. I think a good solution would be the ability to give <br/>\ntextual tags for test classes, and then the ability to specify <br/>\nexpressions such as &quot;!db & !jms&quot; to include/exclude tests with a <br/>\nparticular tag.<br/>\n<br/>\n<br/>\n## Order of Tests ##<br/>\n<br/>\nI think this is a very testing framework specific aspect - how to <br/>\ndeclare the execution order or dependencies between tests. When <br/>\neverything is single-threaded, then the testing framework should already <br/>\nbe in full control of the order of executing things. But when parallel <br/>\ntest execution is added to the mix, some support from the test runner <br/>\nmight be needed, for example to say that some test classes should not be <br/>\nrun in parallel (because of e.g. a shared database).<br/>\n<br/>\nFor Jumi I&#39;m planning on implementing support for restricting the <br/>\nparallelism by annotating test classes (currently everything is in <br/>\nparallel). I&#39;m thinking about having three options: (1) run each test <br/>\nmethod in parallel, (2) run tests methods in this class sequentially but <br/>\nallow other test classes to be run in parallel with it, (3) run this <br/>\ntest class sequentially and don&#39;t run anything else in parallel with it. <br/>\nI think #1 and #3 will at least be needed, but for #2 I&#39;m wondering <br/>\nwhether it should be made more generic by using test groups, for example <br/>\n&quot;don&#39;t run tests tagged with &#39;db&#39; in parallel with any other tests <br/>\ntagged with &#39;db&#39;&quot;.<br/>\n<br/>\nAnd as to what comes to execution order within one test class, for <br/>\nexample TestNG&#39;s dependent methods <br/>\n(<a rel=\"nofollow\" target=\"_blank\" href=\"http://testng.org/doc/documentation-main.html#dependent-methods)\">http://testng.org/doc/documentation-main.html#dependent-methods)</a>, I <br/>\nthink Jumi is already generic enough: There the testing framework starts <br/>\ntest method executions by submitting them to an <br/>\njava.util.concurrent.Executor and it is possible for the testing <br/>\nframework to delay submitting some test to the Executor until all the <br/>\ntests it depends on have finished executing.<br/>\n<br/>\n-- <br/>\nEsko Luontola<br/>\nwww.orfjackal.net</div>", 
    "prevInTime": 24606, 
    "specialLinks": [], 
    "contentTrasformed": false, 
    "postDate": "1414086316", 
    "canDelete": false, 
    "nextInTopic": 24608, 
    "prevInTopic": 24606, 
    "headers": {
        "inReplyToHeader": "PG0yYTJqYStsN29qcnBAWWFob29Hcm91cHMuY29tPg==", 
        "messageIdInHeader": "PDU0NDkzRUFDLjUwNDAzMDBAZ21haWwuY29tPg==", 
        "referencesHeader": "PG0yYTJqYStsN29qcnBAWWFob29Hcm91cHMuY29tPg=="
    }
}