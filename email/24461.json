{
    "numMessagesInTopic": 13, 
    "nextInTime": 24462, 
    "senderId": "XXN8jEVdcEaJN5x3CQm4MIYNStZw5lEu18ApdTWCHnRfy_srjPfL9atB1BW5Gh2bCEp_L3gvYfi6UIGafM1CcxQGDdDlGGY2mm9jynjQ5PhuskPAkwTysYo", 
    "systemMessage": false, 
    "subject": "Re: [junit] RE: Tool to create assertions from a running Java application", 
    "from": "Stephen Connolly &lt;stephen.alan.connolly@...&gt;", 
    "authorName": "Stephen Connolly", 
    "msgSnippet": "... And I often am a bold boy and write one test case called `smokes` with 50-60 assertThat statements in a row... but, and this is the important bit, I know I", 
    "msgId": 24461, 
    "profile": "stephenalanconnolly", 
    "topicId": 24453, 
    "spamInfo": {
        "reason": "12", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 458231986, 
    "messageBody": "<div id=\"ygrps-yiv-1044816995\"><div dir=\"ltr\"><br><div class=\"ygrps-yiv-1044816995gmail_extra\"><br><br><blockquote><span title=\"qreply\"><div class=\"ygrps-yiv-1044816995gmail_quote\">On 4 February 2014 00:58, Teemu Kanstrén <span dir=\"ltr\">&lt;<a rel=\"nofollow\" target=\"_blank\" href=\"mailto:tkanstren@...\">tkanstren@...</a>&gt;</span> wrote:<br>\n<blockquote class=\"ygrps-yiv-1044816995gmail_quote\" style=\"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex;\">\n\n\n\n\n\n\n        \n\n\n\n\n<div>\n\n\n\n\n\n<br><br>\n\n\n\n\n<div dir=\"ltr\"><div class=\"ygrps-yiv-1044816995gmail_extra\"><br><div class=\"ygrps-yiv-1044816995gmail_quote\"><div class=\"ygrps-yiv-1044816995im\">On 3 February 2014 07:57, Stephen Connolly <span dir=\"ltr\">&lt;<a rel=\"nofollow\" target=\"_blank\" href=\"mailto:stephen.alan.connolly@...\">stephen.alan.connolly@...</a>&gt;</span> wrote:<br>\n\n<blockquote class=\"ygrps-yiv-1044816995gmail_quote\" style=\"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex;\">\n\n\n<u></u>\n\n\n\n\n\n\n\n\n\n\n<div>\n<span> </span>\n\n\n<div>\n  <div>\n\n\n    <div>\n      \n      \n      <p></p><div dir=\"ltr\"><div>Quality tests are about meaningful test names and small test case size with at most 1-2 asserts per test case.<br></div><div><br></div></div></div></div></div></div></blockquote><div><br>\n\n</div></div><div>Nice ideal. In practice having a good coverage with at least somehow meaningful tests would be often a good start.. Just saying.</div></div></div></div></div></blockquote><div><br></div><div>And I often am a bold boy and write one test case called `smokes` with 50-60 assertThat statements in a row... but, and this is the important bit, I know I am storing up technical debt by not factoring those tests out into individual test cases and it is a deliberate decision that I make to just write the one smoke test-case &quot;for now&quot;.</div>\n<div><br></div><div>We need to keep the ideal in our head so that we can remember to strive for it.</div><div> </div><blockquote class=\"ygrps-yiv-1044816995gmail_quote\" style=\"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex;\">\n<div><div dir=\"ltr\"><div class=\"ygrps-yiv-1044816995gmail_extra\"><div class=\"ygrps-yiv-1044816995gmail_quote\"><div class=\"ygrps-yiv-1044816995im\"><div><br></div><blockquote class=\"ygrps-yiv-1044816995gmail_quote\" style=\"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex;\">\n\n<div><div><div><div><div dir=\"ltr\"><div>The end effect of that tooling is that the entire code base is frozen in place, as any change results in a complete cascade of test failures... and you run off analysing each and every one just to determine whether the change is the intended effect and the test needs updating, or whether the test is right and you broke something you shouldn&#39;t have.<br>\n\n</div></div></div></div></div></div></blockquote><div><br></div></div><div>I think you just made a great usecase for implementation based test generators. Re-generate after each change and show off your 100% coverage. Even if the tests are pointless, manager is happy :) </div>\n</div></div></div></div></blockquote><div><br></div><div>Until somebody points out that your test cases are not checked into source control... I&#39;ve been down that path before... never ends well...</div><div><br></div>\n<div>There was even the case where we were driving an effort to bump code coverage of a legacy code base from 66% to 75%...</div><div><br></div><div>&quot;Bob&quot; was the database guy, he made a change to some infrastructure stuff that broke 5 of the test cases (we had about 10,000 test cases) on the Friday when going on vacation for two weeks. So our build was unstable as none of us knew exactly how to fix those test cases or how to fix the infrastructure they were testing against... but he&#39;d be back in 2 weeks... we were writing away our tests.</div>\n<div><br></div><div>The Friday before &quot;Bob&quot; came back we had our sprint review with management, they looked at the Code coverage graph in Jenkins... it was sitting nice and near our end-game target at 73%... pats on back all round.</div>\n<div><br></div><div>&quot;Bob&quot; comes back, fixes the infrastructure (some SQL Server permission issue), and we&#39;re back to a &quot;stable&quot; build. (I should point out that everyone was using local code coverage tooling to write *targeted* tests... we had a list of the least covered classes and you would just focus on writing *unit* tests for that class, so nobody was looking at the test coverage graph...</div>\n<div><br></div><div>Those 5 failing test cases were tripping every single exception path that the regular test cases were missing... When &quot;Bob&quot; fixed the database, the code coverage dropped from 73% back down to 66.4%</div>\n<div><br></div><div>Explaining that to management, i.e. a 6 week concentrated concerted effort of the entire team to drive test coverage from 66% to 75% only actually gained us 0.4% extra coverage...</div><div><br></div><div>\nWell if you have ever wondered why the default of the Jenkins Cobertura plugin is to ignore unstable builds... that experience is the reason.</div><div><br></div><div>We have to live in the real world, where you sometimes get managers (or worse still - the boss of your manager) who do not understand the technical things... </div>\n<div> </div><blockquote class=\"ygrps-yiv-1044816995gmail_quote\" style=\"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex;\"><div><div dir=\"ltr\"><div class=\"ygrps-yiv-1044816995gmail_extra\"><div class=\"ygrps-yiv-1044816995gmail_quote\">\n<div class=\"ygrps-yiv-1044816995im\">\n<div><br></div><blockquote class=\"ygrps-yiv-1044816995gmail_quote\" style=\"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex;\"><div><div><div><div><div dir=\"ltr\"><div><br>\n</div><div>automated tooling will not produce quality test cases (unless they are processing an independently derived specification... in which case who tests the specification to be correct... Quis custodiet ipsos custodes?)</div>\n\n\n<div><br></div></div></div></div></div></div></blockquote><div><br></div></div><div>This is how techniques like model-based testing work. In my view your &quot;independently derived specification&quot; is in such case tested by the implementation. Because your implementation is from the viewpoint of your tests another &quot;independently derived specification&quot;. When you generate tests and run those, they will tell you if there is a mismatch. Which of your interpretation of system intent is at fault is then to be investigated..</div>\n</div></div></div></div></blockquote><div><br></div><div>Quis custodiet ipsos custodes?</div><div><br></div><div>Who will guard the guards?</div><div><br></div><div>If you generate code tests from a specification, that is great. We now know that the code implements the specification correctly... we have guards that check the code... but who will check that the guards are enforcing the correct things?</div>\n<div><br></div><div>I would argue that it is somewhat easier to verify code than verify a specification... at least for a lot of the specifications you will see out there.</div><div><br></div><div>Some specifications are use-case based, with wooly descriptions of behaviour. These tend to generate example based test-cases... which give good confidence that the use-case(s) are met... but no confidence when you walk off the tested path.</div>\n<div><br></div><div>Some specifications mix requirements with design, e.g. the UML based type.</div><div><br></div><div>I think you can generate test cases for a parser based on the specification of the grammar that it is required to parse.</div>\n<div><br></div><div>I think you can generate test cases for a performance specification...</div><div><br></div><div>But most of the rest of the time the specification is just too high a level to be useful for generating *unit* tests... unless you are dealing with very junior developers who have been given a rigid spec for the 10 line method that they need to write... and I would argue that the architect should just have written the 10 lines themselves rather than the 3 page spec ;-)</div>\n<div><br></div><div>-Stephen</div><div> </div><blockquote class=\"ygrps-yiv-1044816995gmail_quote\" style=\"margin:0px 0px 0px 0.8ex;border-left-width:1px;border-left-color:rgb(204,204,204);border-left-style:solid;padding-left:1ex;\"><div><div dir=\"ltr\">\n<div class=\"ygrps-yiv-1044816995gmail_extra\"><div class=\"ygrps-yiv-1044816995gmail_quote\"><span class=\"ygrps-yiv-1044816995\"><font color=\"#888888\">\n<div><br></div><div>-Teemu </div></font></span></div></div></div><div class=\"ygrps-yiv-1044816995im\">\n\n\n\n\n\n\n<br>\n\n\n<br>\n\n\n\n\n<div style=\"color:white;clear:both;\"></div>\n</div></div>\n\n\n</blockquote></div><br></span></blockquote></div></div>\n</div>", 
    "prevInTime": 24460, 
    "specialLinks": [], 
    "contentTrasformed": false, 
    "postDate": "1391506201", 
    "canDelete": false, 
    "nextInTopic": 24462, 
    "prevInTopic": 24460, 
    "headers": {
        "inReplyToHeader": "PENBR2FnTD1kSEdUUlFlTk1faVk1TVZYV2hoWTdqcDNVMjdiQThMQjkzTDBSY3RtN2tSQUBtYWlsLmdtYWlsLmNvbT4=", 
        "messageIdInHeader": "PENBK25Qbk13ZENtX1c9YWYzeHVWbVpxMTVmS1JYdDZTVGR2VXJTelhRVFdFbXZDb1Vnd0BtYWlsLmdtYWlsLmNvbT4=", 
        "referencesHeader": "PGxjaXRxZysxaHQ1dTNpQFlhaG9vR3JvdXBzLmNvbT4JPDUyRUQ1NUY1LjgwMzA2MDJAZ21haWwuY29tPgk8bGNuaDdjKzFna21qajRAWWFob29Hcm91cHMuY29tPgk8Q0ErblBuTXpINFF5bVlzeXphYnRvZ3NET0ZPZGJqUkwtcnVKQnNhU3pkS3dnX3gzbjlRQG1haWwuZ21haWwuY29tPgk8Q0FHYWdMPWRIR1RSUWVOTV9pWTVNVlhXaGhZN2pwM1UyN2JBOExCOTNMMFJjdG03a1JBQG1haWwuZ21haWwuY29tPg=="
    }
}