{
    "numMessagesInTopic": 10, 
    "nextInTime": 23778, 
    "senderId": "N2wWmO9gYHObNeGUcPQi9B297eixEyKiqS0sJOp92xSMtVLKqspPUsiKWLLsRTF57OIzwFa59S0oaoRDtYrT8YFnQkLfe3jW", 
    "systemMessage": true, 
    "subject": "Re: Reporting on test runs from a custom Runner", 
    "from": "&quot;sschroev&quot; &lt;stephan202@...&gt;", 
    "authorName": "sschroev", 
    "msgSnippet": "Hi Jonathan, Out of curiosity: have you considered implementing your framework extension as a TestRule instead of a Runner? Cheers, Stephan", 
    "msgId": 23777, 
    "profile": "sschroev", 
    "topicId": 23768, 
    "spamInfo": {
        "reason": "0", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 495664717, 
    "messageBody": "<div id=\"ygrps-yiv-740544154\">Hi Jonathan,<br/>\n<br/>\nOut of curiosity: have you considered implementing your framework extension as a TestRule instead of a Runner?<br/>\n<br/>\nCheers,<br/>\nStephan<br/>\n<br/>\n<blockquote><span title=\"qreply\"> --- In <a rel=\"nofollow\" target=\"_blank\" href=\"mailto:junit@yahoogroups.com\">junit@yahoogroups.com</a>, &quot;jfuerth&quot; &lt;jfuerth@...&gt; wrote:<br/>\n&gt;<br/>\n&gt; Hi everyone,<br/>\n&gt; <br/>\n&gt; I&#39;ve created a custom Runner meant to help with performance and scalability testing. It lets you create tests like this:<br/>\n&gt; <br/>\n&gt; @RunWith(PerfRunner.class)<br/>\n&gt; public class MultiThreadUsageExampleTest {<br/>\n&gt; <br/>\n&gt;   @Test<br/>\n&gt;   public void demonstrationTest(<br/>\n&gt;       @Varying(from=1, to=10) int threadCount,<br/>\n&gt;       @Varying(from=100000, to=1000000, step=100000) int entryCount) {<br/>\n&gt; <br/>\n&gt;       // code here that relies on threadCount and entryCount<br/>\n&gt;   }<br/>\n&gt; <br/>\n&gt; }<br/>\n&gt; <br/>\n&gt; In the above example, my runner calls demonstrationTest() 100 times, with parameter values (1, 100000), (2, 100000), ... (1, 200000), ... (10, 1000000).<br/>\n&gt; <br/>\n&gt; What I want to do now is produce charts of how long each call took. There are a lot of approaches, but I don&#39;t know which is &quot;right.&quot;<br/>\n&gt; <br/>\n&gt; 1. Register a TestListener and write my own reports to disk from it (but is there a generic way to register listeners?)<br/>\n&gt; 2. Create my own main program that runs the test suite, and have it create the report when the suite is done (maybe this is the only way to register a TestListener anyhow?)<br/>\n&gt; 3. Transform the XML output of the ant junit task (but then it&#39;s bound to Ant)<br/>\n&gt; 4. Transform the XML output of the maven-surefure-plugin (but then it&#39;s bound to Maven)<br/>\n&gt; 5. Write performance results to disk directly from my TestRunner (should work in any environment, but feels like a dirty approach)<br/>\n&gt; <br/>\n&gt; Other ideas are welcome too, of course. My test runner is open source and published on GitHub; I want a solution that can work for everyone. So I think ideas 3 and 4 are out.<br/>\n&gt; <br/>\n&gt; Has anyone else thought about getting data from custom test runners into human-readable reports?<br/>\n&gt; <br/>\n&gt; -Jonathan<br/>\n&gt; </span></blockquote></div>", 
    "prevInTime": 23776, 
    "specialLinks": [], 
    "contentTrasformed": false, 
    "postDate": "1324214484", 
    "canDelete": false, 
    "nextInTopic": 0, 
    "prevInTopic": 23776, 
    "headers": {
        "inReplyToHeader": "PGpjYjZjbCszY3JpQGVHcm91cHMuY29tPg==", 
        "messageIdInHeader": "PGpja3Bjayt0NDQwQGVHcm91cHMuY29tPg=="
    }
}