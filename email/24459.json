{
    "numMessagesInTopic": 13, 
    "nextInTime": 24460, 
    "senderId": "h5l-2J6Xz-V6p-Zg4ZsUWWM89XdTLmOEiTnd11snUG59l5OioO8e2lv2lZOfOmI5BNJfTG8uSosBIG3O_mjpj4qXxhHZshe8nVz25kdOYqtBhWS0JTDLAhI", 
    "systemMessage": false, 
    "subject": "Re: [junit] RE: Tool to create assertions from a running Java application", 
    "from": "Stephen Connolly &lt;stephen.alan.connolly@...&gt;", 
    "authorName": "Stephen Connolly", 
    "msgSnippet": "There is a siren that I call 100% code coverage. You hear the siren call most strongly from practitioners in those languages that have dynamic/weak typing, a", 
    "msgId": 24459, 
    "profile": "stephenalanconnolly", 
    "topicId": 24453, 
    "spamInfo": {
        "reason": "12", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 458231986, 
    "messageBody": "<div id=\"ygrps-yiv-1054150462\"><div dir=\"ltr\">There is a siren that I call 100% code coverage.<div><br></div><div>You hear the siren call most strongly from practitioners in those languages that have dynamic/weak typing, a global variable scope and are interpreted.</div>\n<div><br></div><div>When you are dealing with a static/semi-strongly typed language such as Java, which is compiled and does not have a real global variable scope (closest thing is System.getProperties() which can only hold String instances), the compiler itself ensures that 100% of lines are subject to some tests (i.e. namely is it syntactically correct and does it match the typing rules)... on top of which you can add more &quot;tests&quot; using tooling such as findbugs, PMD and checkstyle (assuming you use a zero-failures policy)</div>\n<div><br></div><div>Is that enough tests for Java code... hell no... but you have 100% code coverage... the code coverage you measure with tooling such as cobertura/clover/emma/jacoco is the additional coverage you get from the additional tests you have written.</div>\n<div><br></div><div>When a dynamic typing interpretive language person asks: &quot;are you comfortable releasing your 10,000 lines of code with 1% of those lines not tested? that&#39;s 100 lines without tests?&quot; they are asking because there could be a syntax or type error in one of those 100 lines that causes their code to blow up. I would not be comfortable with such.</div>\n<div><br></div><div>With a compiled static typing language, however, the answer is &quot;yes we do have tests... the compiler gives us tests... they may not be our best tests... but we have tests... shall we take a look at all your tests and see how good they are in order to get to 100% coverage?&quot;</div>\n<div><br></div><div>My point is this:</div><div><br></div><div>With a language like Java, 100% code coverage is not the goal. Quality tests is the goal.</div><div><br></div><div>It is easy to &quot;generate&quot; tests from code... the kind of tests you end up with will verify that the code behaves the same as before... so when you change *anything meaningful* some tests will fail... only now you don&#39;t know why the tests are failing... is it because you changed the code to match the new requirement... or is it because you broke something else in the process?</div>\n<div><br></div><div>Quality tests are about meaningful test names and small test case size with at most 1-2 asserts per test case.</div><div><br></div><div>The test name should tell you what is being tested and why. The 1-2 asserts (should really be just 1) should validate the actual outcome.</div>\n<div><br></div><div>@Test</div><div>void lookUpWidgetById_nullId() {</div><div>  assertThat(instance.lookup(null), is(nullValue());</div><div>}</div><div><br></div><div>@Test</div><div>void lookUpWidgetById_standardWidget() {</div>\n<div>  assertThat(instance.lookup(STANDARD_WIDGET_ID), is(hasProperty(&quot;id&quot;,STANDARD_WIDGET_ID)));</div><div>}</div><div><br></div><div>is a better quality of tests than </div><div><br></div><div><div>@Test</div>\n<div>void lookUpWidgetById() {</div><div>  assertThat(instance.lookup(null), is(nullValue());</div><div>  assertThat(instance.lookup(STANDARD_WIDGET_ID), is(hasProperty(&quot;id&quot;,STANDARD_WIDGET_ID)));<br></div><div>\n}</div></div><div><br></div><div>Similarly</div><div><br></div><div>@Test</div><div>void manchuify_hasFrongles() {</div><div>  Foo instance = new Foo();</div><div>  assertThat(instance.getFrongleCount(), is(0));</div><div>\n  instance.manchuify();</div><div>  assertThat(instance.getFrongleCount(), is(greaterThan(0)));</div><div>}</div><div><br></div><div><div>@Test</div><div>void manchuify_isRed() {</div><div>  Foo instance = new Foo();</div>\n<div>  assertThat(instance.getColor(), not(is(Color.RED)));</div><div>  instance.manchuify();</div><div>  assertThat(instance.getColor(), is(Color.RED));</div><div>}</div><div><br></div></div><div>is better quality than</div>\n<div><br></div><div><div>@Test</div><div>void manchuify() {</div><div>  Foo instance = new Foo();</div><div>  assertThat(instance.getFrongleCount(), is(0));</div><div>  assertThat(instance.getColor(), not(is(Color.RED)));<br>\n</div><div></div><div>  instance.manchuify();</div><div>  assertThat(instance.getFrongleCount(), is(greaterThan(0)));</div><div>  assertThat(instance.getColor(), is(Color.RED));<br></div><div><div>}</div></div></div><div>\n<br></div><div>because by splitting out the tests we know that these are individual requirements... in addition we also know the scope of the problem... if something happens the Foo.manchuify() method that it no longer makes the Foo instances red, in both cases we get two failing tests... but the better quality test tells us exactly what is wrong.</div>\n<div><br></div><div>With automated test generation based off either the compiled code or the running system, the tool cannot tell you why things are the way they are, so hence the tool cannot give the tests good names, and cannot differentiate the related elements... the frongle count being non zero and the colour being red are strongly correlated, so how can the system know that these are a side-effect of the original spec and that the new spec allows for non-red Foo instances after manchuification?</div>\n<div><br></div><div>If you are a smart developer, you will understand all of the above... and you will be saying... this is just a tool... I am the person who will step in and create the context... I just want some automation to take out the grunt work...</div>\n<div><br></div><div>Well that is fine as long as you don&#39;t ever face a manager who fears the even 1 untested line of code in their source release, or who receives an edict from the on-high CTO (who was raised on dynamic interpreted languages) and now has to get to 100% coverage... sees the tooling and sees it as a route to 100% coverage...</div>\n<div><br></div><div>The end effect of that tooling is that the entire code base is frozen in place, as any change results in a complete cascade of test failures... and you run off analysing each and every one just to determine whether the change is the intended effect and the test needs updating, or whether the test is right and you broke something you shouldn&#39;t have.</div>\n<div><br></div><div>It&#39;s not about the % coverage... it&#39;s about the quality of your test cases... automated tooling will not produce quality test cases (unless they are processing an independently derived specification... in which case who tests the specification to be correct... Quis custodiet ipsos custodes?)</div>\n<div><br></div><div>-Stephen</div></div><div class=\"ygrps-yiv-1054150462gmail_extra\"><br><br><blockquote><span title=\"qreply\"> <div class=\"ygrps-yiv-1054150462gmail_quote\">On 3 February 2014 07:41,  <span dir=\"ltr\">&lt;<a rel=\"nofollow\" target=\"_blank\" href=\"mailto:mirko.sertic@...\">mirko.sertic@...</a>&gt;</span> wrote:<br>\n<blockquote class=\"ygrps-yiv-1054150462gmail_quote\" style=\"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex;\">\n\n\n\n\n\n\n        \n\n\n\n\n<div>\n\n\n\n\n\n<br><br>\n\n\n\n\nHi there<br><br>Yes, i totally agree with you: the TDD way is a much more useful way to create software. From my point of view this is the way to go for a greenfield project.<br><br>However, while working with legacy code this is not always possible. The described tool is a simple helper class to create a good test coverage for existing codebases before changing something, so basically it is a TDD helper. Nothing more. Of course it is up to the designer to check what is tested and create a good test strategy. For our current project, this simple helper saved us a lot of time, so i want to share it with the community.<br>\n<br>Regards<br>Mirko\n\n\n\n\n\n<br>\n\n\n<br>\n\n\n\n\n<div style=\"color:white;clear:both;\"></div>\n</div>\n\n\n</blockquote></div><br></span></blockquote></div>\n</div>", 
    "prevInTime": 24458, 
    "specialLinks": [], 
    "contentTrasformed": false, 
    "postDate": "1391432278", 
    "canDelete": false, 
    "nextInTopic": 24460, 
    "prevInTopic": 24458, 
    "headers": {
        "inReplyToHeader": "PGxjbmg3YysxZ2ttamo0QFlhaG9vR3JvdXBzLmNvbT4=", 
        "messageIdInHeader": "PENBK25Qbk16SDRReW1Zc3l6YWJ0b2dzRE9GT2RialJMLXJ1SkJzYVN6ZEt3Z194M245UUBtYWlsLmdtYWlsLmNvbT4=", 
        "referencesHeader": "PGxjaXRxZysxaHQ1dTNpQFlhaG9vR3JvdXBzLmNvbT4JPDUyRUQ1NUY1LjgwMzA2MDJAZ21haWwuY29tPgk8bGNuaDdjKzFna21qajRAWWFob29Hcm91cHMuY29tPg=="
    }
}