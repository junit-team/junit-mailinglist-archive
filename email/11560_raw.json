{
    "numMessagesInTopic": 28, 
    "nextInTime": 11561, 
    "senderId": "fim-oWyve8FH3FRl7dqfMGYhBiJlSreQ-9qwDUoZzwSc8uBaHtwQkeN-2Zd-4zFUZbQhHPgS6i4v3Ovi5oon2W3McaTAxtIg3aWsPbpItQ", 
    "systemMessage": false, 
    "subject": "Re: [junit] a test framework to codify levelizaton?", 
    "from": "&quot;J. B. Rainsberger&quot; &lt;jbrains@...&gt;", 
    "authorName": "J. B. Rainsberger", 
    "msgSnippet": "... Can you quantify this? GSBase, for example, runs tests at approximately 100/second, and that s my usual performance goal for tests. On my most recent", 
    "msgId": 11560, 
    "rawEmail": "Return-Path: &lt;jbrains@...&gt;\r\nX-Sender: jbrains@...\r\nX-Apparently-To: junit@yahoogroups.com\r\nReceived: (qmail 78325 invoked from network); 17 Jul 2004 04:03:59 -0000\r\nReceived: from unknown (66.218.66.216)\n  by m24.grp.scd.yahoo.com with QMQP; 17 Jul 2004 04:03:59 -0000\r\nReceived: from unknown (HELO smtp101.rog.mail.re2.yahoo.com) (206.190.36.79)\n  by mta1.grp.scd.yahoo.com with SMTP; 17 Jul 2004 04:03:59 -0000\r\nReceived: from unknown (HELO ?192.168.1.119?) (jbrains@24.156.43.226 with plain)\n  by smtp101.rog.mail.re2.yahoo.com with SMTP; 17 Jul 2004 04:03:44 -0000\r\nMessage-ID: &lt;40F8A4F7.8090506@...&gt;\r\nDate: Sat, 17 Jul 2004 00:03:03 -0400\r\nUser-Agent: Mozilla Thunderbird 0.7.1 (Windows/20040626)\r\nX-Accept-Language: en-us, en\r\nMIME-Version: 1.0\r\nTo: junit@yahoogroups.com\r\nReferences: &lt;20040716210759.53219.qmail@...&gt;\r\nIn-Reply-To: &lt;20040716210759.53219.qmail@...&gt;\r\nContent-Type: text/plain; charset=us-ascii; format=flowed\r\nContent-Transfer-Encoding: 7bit\r\nX-eGroups-Remote-IP: 206.190.36.79\r\nFrom: &quot;J. B. Rainsberger&quot; &lt;jbrains@...&gt;\r\nSubject: Re: [junit] a test framework to codify levelizaton?\r\nX-Yahoo-Group-Post: member; u=127224993\r\nX-Yahoo-Profile: nails762\r\n\r\nVlad Roubtsov wrote:\n\n&gt; On any but the trivial projects the number of\n&gt; testcases quickly exceeds 100-200+. It usually takes a\n&gt; noticeable time to run them all.\n\nCan you quantify this? GSBase, for example, runs tests at approximately \n100/second, and that&#39;s my usual performance goal for tests. On my most \nrecent project, I separated tests that required expensive, external \nresources (there were about 10) from tests than ran entirely in memory \n(there were about 400). The in-memory tests ran in about 6 seconds.\n\nIs this &quot;noticeable&quot; by your standards? I&#39;d merely like some extra context.\n\n &gt; It is also less\n&gt; efficient to do so because it ignores the idea of\n&gt; levelization (from John Lakos&#39; underappreciated book\n&gt; &quot;Large Scale C++ Software Design&quot;). In incremental dev\n&gt; mode it would be more efficient to execute only the\n&gt; testcases that correspond to the product package(s)\n&gt; that&#39;s just changed (and the dependency closure\n&gt; thereof) AND execute them in increasing level numbers\n&gt; (reverse package dependency topological order).\n\nThis is a great idea, however, computing the complete dependency closure \nis awfully tricky. For most algorithms that attempt to do this, I can \nload a class through reflection and use it, breaking the algorithm&#39;s \nassertion that it correctly computes the runtime dependency closure. I&#39;m \nnot saying it&#39;s impossible -- although it might be -- but I am doubtful \nthat any existing tool does this adequately and could be integrated into \nEclipse, for example.\n\n&gt; A tool/framework that enforces test (sub)suite\n&gt; execution in increasing level numbers would seem like\n&gt; the next evolutionary step after naive TDD. Does\n&gt; anyone know of such a tool? The testcase-&gt;app package\n&gt; assignment could be done by simply placing testcases\n&gt; in the relevant package (common practice already\n&gt; anyway) or by something like Java 1.5 metadata\n&gt; tagging.\n\nI&#39;m not aware of anything, but if it exists, the C#/.NET world would be \na likely first place to look. They&#39;ve had metadata tagging longer than \nJava folks have.\n\n&gt; There are tools to analyze existing package\n&gt; dependencies, but I am not aware of a test framework\n&gt; to actually relate that data to testcase execution\n&gt; ordering.\n\nThose tools likely only analyze compile-time (structural) dependecies, \nand as I wrote above, it would be all too easy to exhibit a runtime \ndependency that such an analyzer would miss. The result is that runtime \nbehavior might change without the analyzer knowing to run all the \nappropriate tests.\n\nI am admittedly shaky on the theory, and welcome being proven wrong, but \nI have this vague notion that I&#39;m right.\n-- \nJ. B. Rainsberger,\nDiaspar Software Services\nhttp://www.diasparsoftware.com :: +1 416 791-8603\nLet&#39;s write software that people understand\n\n", 
    "profile": "nails762", 
    "topicId": 11558, 
    "spamInfo": {
        "reason": "0", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 127224993, 
    "prevInTime": 11559, 
    "contentTrasformed": false, 
    "postDate": "1090036983", 
    "canDelete": false, 
    "nextInTopic": 11561, 
    "prevInTopic": 11558, 
    "headers": {
        "inReplyToHeader": "PDIwMDQwNzE2MjEwNzU5LjUzMjE5LnFtYWlsQHdlYjUwMzA2Lm1haWwueWFob28uY29tPg==", 
        "messageIdInHeader": "PDQwRjhBNEY3LjgwOTA1MDZAcm9nZXJzLmNvbT4=", 
        "referencesHeader": "PDIwMDQwNzE2MjEwNzU5LjUzMjE5LnFtYWlsQHdlYjUwMzA2Lm1haWwueWFob28uY29tPg=="
    }
}