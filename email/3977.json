{
    "numMessagesInTopic": 9, 
    "nextInTime": 3978, 
    "senderId": "5mfEE2XhvNSzkxshETaylEtCjtCjTmR8apu7s1MA0_jA6lZcHFtk19I0SxSXXGm9vOlRItAPp3cQ58-yIZvpGsTe1MP0fBsB0ze9yCivnA", 
    "systemMessage": false, 
    "subject": "Re: How to get fixture from TestSetup", 
    "from": "Emily Bache &lt;emily.bache@...&gt;", 
    "authorName": "Emily Bache", 
    "msgSnippet": "... Hi, I am currently co-writing a practitioners report submission for the XP2002 conference about our[*] experiences with acceptance testing. We ve got a ", 
    "msgId": 3977, 
    "profile": "emilybache", 
    "topicId": 3909, 
    "spamInfo": {
        "reason": "0", 
        "isSpam": false
    }, 
    "replyTo": "LIST", 
    "userId": 53044859, 
    "messageBody": "<div id=\"ygrps-yiv-1781458352\"><blockquote><span title=\"ireply\">&gt; <br/>\n&gt;    From: Shane Celis &lt;<a rel=\"nofollow\" target=\"_blank\" href=\"mailto:shane@...\">shane@...</a>&gt;<br/>\n&gt; Many times over JUnit seems to be employed for <br/>\n&gt; functional/acceptance/integration testing (usually somewhat <br/>\n&gt; awkwardly).  <br/>\n&gt; Are there any testing approaches or frameworks that can be <br/>\n&gt; suggested for <br/>\n&gt; this sort of testing?  Because many of the methodologies <br/>\n&gt; suggested for <br/>\n&gt; unit testing aren&#39;t applicable to this sort of testing.  Have <br/>\n&gt; the xUnit <br/>\n&gt; people, or anyone else for that matter, addressed this problem space?<br/>\n<br/>\n<br/>\n </span></blockquote>Hi,<br/>\n<br/>\nI am currently co-writing a practitioners report submission for the XP2002<br/>\nconference about our[*] experiences with acceptance testing. We&#39;ve got a<br/>\ntesting framework that&#39;s not based on JUnit at all, and we think it&#39;s<br/>\nworking very well.<br/>\n<br/>\nWe force the application to operate in batch mode while we acceptance test<br/>\nit, provide input from file/standard input redirects, and collect all output<br/>\nas text files. We then have the full power of unix text processing commands<br/>\n(ie diff) to analyse and compare the output to previous runs. Basically, if<br/>\nthe output is different, the test fails.<br/>\n<br/>\nThe testing framework takes responsibility for version controlling input and<br/>\noutput for each test case, and running the application executable with said<br/>\ninput, and checking the output for differences (ie failures). The test cases<br/>\nrun in parallel over a network, using some third party load balancing<br/>\nsoftware.<br/>\n<br/>\nThe main advantages of this approach are that you&#39;re not writing any code to<br/>\nadd a new test case, only supplying new input data and recording a standard<br/>\nresult. Secondly the tests run in parallel instead of in series, so if the<br/>\nload on your network is low, you get results faster, and often all the<br/>\nresults in the time it takes the slowest test to run. <br/>\n<br/>\nThe disadvantages are that you have to force your app to work in batch mode<br/>\nand produce text output, which may or may not be straightforward. <br/>\n<br/>\nHope that might give you some ideas for acceptance testing.<br/>\n<br/>\nEmily<br/>\n<br/>\n[*] the XP team at Carmen Systems, which I am not actually a member of, but<br/>\nI know a lot about.</div>", 
    "prevInTime": 3976, 
    "specialLinks": [], 
    "contentTrasformed": false, 
    "postDate": "1012999390", 
    "canDelete": false, 
    "nextInTopic": 3978, 
    "prevInTopic": 3959, 
    "headers": {
        "messageIdInHeader": "PDcwMEVBMTdBMTQyQUQzMTE4QTFFMDAwOEM3MjRCQ0I0RUI3Q0VFQGFnYmdudHMwMy5hZ2JnLmludHJhbmV0Pg=="
    }
}